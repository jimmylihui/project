{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmylihui/project/blob/main/Project(pure_code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkLgBiu3G-7W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_excel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "my_sheet = 'Sheet1' # change it to your sheet name, you can find your sheet name at the bottom left of your excel file\n",
        "file_name = 'project.xlsx' # change it to the name of your excel file\n",
        "df = read_excel(file_name, sheet_name = my_sheet)#reading the data\n",
        "df = df.iloc[: , :]#reading the data\n",
        "x=df.values#reading the value:X,Y,MUX,MUY\n",
        "print(x)\n",
        "x = StandardScaler().fit_transform(x)#applying standard scalar to the data\n",
        "\n",
        "a=x[0:20]\n",
        "\n",
        "pca = PCA(n_components=2)#using PCA with component of 2\n",
        "principalComponents = pca.fit_transform(a)#fit and transform the data with PCA\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])#transform the array into datafram\n",
        "print(principalDf)\n",
        "print(pca.transform(a))\n",
        "print(pca.transform(a[0:10]))\n",
        "#transform keep the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OptuPHZrG-7b"
      },
      "outputs": [],
      "source": [
        "print(df.membership)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNhRlZh6G-7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def scatter_plot(plt, x_data, y_data, x_label,y_label,title0):\n",
        "  \"\"\"\n",
        "    scatter_plot draw a 2-dimensional scatter plot.\n",
        "\n",
        "    :param plt: describe plot function\n",
        "    :param x_data: describe data of x \n",
        "    :param y_data: describe data of y \n",
        "    :param x_label: describe label of x \n",
        "    :param y_label: describe label of y \n",
        "    :param title0: describe title of plot \n",
        "    :return: none\n",
        "  \"\"\" \n",
        "  plt.scatter(x_data,y_data)#plot the data with X and Y\n",
        "  plt.xlabel(x_label)\n",
        "  plt.ylabel(y_label)\n",
        "  plt.title(title0)\n",
        "\n",
        "def scatter_plot_c(plt,x_data,y_data,x_label,y_label,title0,c0,cmap0,classes):\n",
        "  \"\"\"\n",
        "    scatter_plot draw a 2-dimensional scatter plot.\n",
        "\n",
        "    :param plt: describe plot function\n",
        "    :param x_data: describe data of x \n",
        "    :param y_data: describe data of y \n",
        "    :param x_label: describe label of x \n",
        "    :param y_label: describe label of y \n",
        "    :param title0: describe title of plot \n",
        "    :param c0: describe classification\n",
        "    :param cmap0: describe color of classification\n",
        "    :param classes: describe label of classification\n",
        "    :return: none\n",
        "  \"\"\" \n",
        "  scatter=plt.scatter(x_data,y_data,c=c0,cmap=cmap0)#plot the data with X and Y\n",
        "  plt.xlabel(x_label)\n",
        "  plt.ylabel(y_label)\n",
        "  plt.title(title0)\n",
        "  plt.legend(handles=scatter.legend_elements()[0],labels=classes)\n",
        "\n",
        "scatter_plot(plt,df.X,df.Y,\"x position\",\"y position\",\"visualization of data with position information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhBZKop3G-7f"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "colors = ListedColormap(['b','r'])\n",
        "\n",
        "classes = ['non-cluster membership [1]', 'cluster membership [1]']#red data means cluster member while blue data means non-cluster member\n",
        "\n",
        "scatter_plot_c(plt, df.X, df.Y, \"x position (mm)\",\"y position (mm)\",\"visualization of data with position information\",df.membership,colors,classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etkop6uRG-7g"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_centroid(x_data,y_data):\n",
        "  \"\"\"\n",
        "    find_centroid find the center of the 2-dimensional data.\n",
        "\n",
        "    :param x_data: describe data of x \n",
        "    :param y_data: describe data of y \n",
        "    :return: minimum_distance: describe the total distance from centre to other point\n",
        "    :return: minimum_number:discribe the number of centre\n",
        "    :return: position:discribe the position of centre\n",
        "  \"\"\" \n",
        "  center_point_number=1\n",
        "  minimum_distance=0\n",
        "  minimum_number=1\n",
        "  for n in range(0, len(x_data)):#this loop is to find the point who has the smallest total distance to all other point\n",
        "    first_point_x=x_data[n]\n",
        "    first_point_y=y_data[n]\n",
        "    total_distance=0\n",
        "    for m in range(0, len(x_data)):\n",
        "      second_point_x=x_data[m]\n",
        "      second_point_y=y_data[m]\n",
        "      total_distance=total_distance+(first_point_x-second_point_x)**2+(first_point_y-second_point_y)**2\n",
        "    if n==1:\n",
        "      minimum_distance=total_distance\n",
        "    elif total_distance<minimum_distance:\n",
        "      minimum_distance=total_distance \n",
        "      minimum_number=n\n",
        "  position=(x_data[minimum_number],y_data[minimum_number])\n",
        "  return minimum_number,minimum_distance,position\n",
        "\n",
        "minimum_number,minimum_distance,centroid=find_centroid(df.X,df.Y)\n",
        "print(minimum_distance)\n",
        "\n",
        "\n",
        "print(df.NR[minimum_number])#the point of having smallest total distance is the center point\n",
        "\n",
        "\n",
        "print(centroid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj14z0ixG-7i"
      },
      "outputs": [],
      "source": [
        "#draw zone 1 to zone 4\n",
        "zone1=[];\n",
        "zone1_number=[];\n",
        "zone2=[];\n",
        "zone3=[];\n",
        "zone4=[];\n",
        "zone2_number=[];\n",
        "zone3_number=[];\n",
        "zone4_number=[];\n",
        "\n",
        "for n in range(0, len(df.X)):#this loop is to find the No of star in each zoo according to their distance to the centeroid\n",
        "  distance=(df.X[n]-centroid[0])**2+(df.Y[n]-centroid[1])**2\n",
        "  \n",
        "  if (distance<25):\n",
        "    zone1.append(df.NR[n])\n",
        "    zone1_number.append(n);\n",
        "  elif(distance<49.9849):\n",
        "    zone2.append(df.NR[n])\n",
        "    zone2_number.append(n);\n",
        "  elif(distance<74.9956):\n",
        "    zone3.append(df.NR[n])\n",
        "    zone3_number.append(n);\n",
        "  elif(distance<100):\n",
        "    zone4.append(df.NR[n])\n",
        "    zone4_number.append(n);\n",
        "print(zone1)\n",
        "print(zone2)\n",
        "print(zone3)\n",
        "print(zone4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot zone 1 based on position\n",
        "zone1_x_axis=[];\n",
        "zone1_y_axis=[];\n",
        "zone1_label=[];\n",
        "for n in zone1_number:#this loop finding the X, Y and members of stars in Zone 1\n",
        "  zone1_x_axis.append(df.X[n])\n",
        "  zone1_y_axis.append(df.Y[n])\n",
        "  zone1_label.append(df.membership[n])\n",
        "\n",
        "scatter_plot_c(plt,zone1_x_axis,zone1_y_axis,\"x position\",\"y position\",\"visualization of zone 1 data with position information\",zone1_label,colors,classes)"
      ],
      "metadata": {
        "id": "Ytf7EH3fpayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u11sGTKhG-7j"
      },
      "outputs": [],
      "source": [
        "#applying pca to zone 1\n",
        "zone1_values=[]\n",
        "zone1_tag=[]\n",
        "\n",
        "for n in zone1_number:#this loop save the values:X Y MUX MUY of starts in Zone1 to zone1.values, and membership of that to zone1.tag\n",
        "  zone1_values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "  zone1_tag.append(df.membership[n])\n",
        "print(zone1_values)\n",
        "\n",
        "scaler = StandardScaler()#setting up standard scalar, standard scaler standardize features by removing the mean and scaling to unit variance.\n",
        "scaler.fit(zone1_values)#fit the standard scalar with values in Zone1\n",
        "\n",
        "zone1_values=scaler.transform(zone1_values)#applying standard scalar to values in Zone1\n",
        "# zone1_values = StandardScaler().fit_transform(zone1_values)\n",
        "\n",
        "print(zone1_tag)\n",
        "\n",
        "# cumsum=np.cumsum(pca.explained_variance_ratio_)\n",
        "# d=np.argmax(cumsum>=0.95)+1\n",
        "\n",
        "pca=PCA(n_components=2)#initializing the PCA\n",
        "pca.fit_transform(zone1_values)#applying the PCA to the values of zone 1 \n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)#seting up kernel PCA with linear Kernel\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04, fit_inverse_transform=True)#seting up kernel PCA with rbf Kernel\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)#seting up kernel PCA with sigmoid Kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-RyjCJVG-7k"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        " \n",
        "#loop over methods and plot the projection of the Swiss roll\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.xlabel(\"principal component 1\")\n",
        "plt.ylabel(\"principal component 2\")\n",
        "plt.suptitle(\"visualization of data with PCA information\")\n",
        "colormap = np.array(['b', 'r'])\n",
        "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), #plot datas after applying linear kernel, RBF kernel and sigmoid kernel\n",
        "                            (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n",
        "                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
        "    X_reduced = pca.fit_transform(zone1_values)\n",
        "    if subplot == 132:\n",
        "        X_reduced_rbf = X_reduced\n",
        "    \n",
        "    plt.subplot(subplot), plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=colormap[zone1_tag])\n",
        "    plt.xlabel(\"principal component 1\",fontsize=16)\n",
        "    plt.ylabel(\"principal component 2\",fontsize=16)\n",
        "    plt.title(format(title), fontsize=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glXV9E3rG-7l"
      },
      "source": [
        "finding the hyperparameter to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx85Evb6G-7l"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "clf = Pipeline([#setting up the basic parameter of PCA\n",
        "        (\"kpca\", KernelPCA(n_components=2)),\n",
        "        (\"log_reg\", LogisticRegression())\n",
        "    ])\n",
        "\n",
        "param_grid = [{#setting up the different options of kernel and gamma of PCA\n",
        "        \"kpca__gamma\": np.linspace(0.0001, 0.05, 10),\n",
        "        \"kpca__kernel\": [\"rbf\", \"sigmoid\",\"linear\"]\n",
        "    }]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)#finding the best parameter for gamma and PCA\n",
        "grid_search.fit(zone1_values, zone1_tag)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J02T-mzBG-7n"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "gamma1, gamma2 = 0.1, 10 # giving 2 options of gamma, 0.1,10\n",
        "C1, C2 = 0.001, 1000 #giving 2 otions of C 0.001 and 1000\n",
        "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2) #setting up 4 combinations of gamma and C\n",
        "\n",
        "svm_clfs = []\n",
        "for gamma, C in hyperparams:  #for each pair of gamma and C, applying them to rbf kernel and then applying kernal to PCA\n",
        "    rbf_kernel_svm_clf = Pipeline((\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
        "        ))\n",
        "    rbf_kernel_svm_clf.fit(X_reduced, zone1_tag)\n",
        "    svm_clfs.append(rbf_kernel_svm_clf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf1mK_L2G-7n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this function predict the contour with different pair of gamma and C\n",
        "# parameter {clf} array including pipline Pipeline((\n",
        "#            (\"scaler\", StandardScaler()),\n",
        "#            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
        "#        ))\n",
        "# parameter {axes} array setting up the range to predict\n",
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
        "    \n",
        "# this function plot contour\n",
        "# parameter X array the values of star\n",
        "# parameter y array the label of star\n",
        "# parameter axes array the range of plot\n",
        "def plot_dataset(X, y, axes):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colormap[y])\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(\"principal component 1\", fontsize=10)\n",
        "    plt.ylabel(\"principal component 2\", fontsize=10 )\n",
        "    \n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "for i, svm_clf in enumerate(svm_clfs):#plot result with each pair of gamma and C\n",
        "    plt.subplot(221 + i)\n",
        "    plot_predictions(svm_clf, [-0.05, 0.05, -0.05, 0.05])\n",
        "    plot_dataset(X_reduced, zone1_tag, [-0.05, 0.05, -0.05, 0.05])\n",
        "    gamma, C = hyperparams[i]\n",
        "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
        "plt.suptitle('visualization of boundary in zone 1 with different pair of gamma and C')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using half data as training data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( \n",
        "    X_reduced, zone1_tag, test_size=0.33, random_state=42)#splite the processed data in zone1 into 67% training data, 33% of testing data\n",
        "\n",
        "rbf_kernel_svm_clf = Pipeline((\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=0.1, C=1000))#setting up gamma=0.1, C=100\n",
        "        ))\n",
        "rbf_kernel_svm_clf.fit(X_train, y_train) #fit the boundary\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.05, 0.05, -0.05, 0.05]) #plot boundary\n",
        "plot_dataset(X_train, y_train, [-0.05, 0.05, -0.05, 0.05]) # plot datapoint\n",
        "plt.title(\"visualization of boundary in zone 1 with gamma=0.1, C=1000\")\n",
        "y_test_pred = rbf_kernel_svm_clf.predict(X_test)\n",
        "\n",
        "\n",
        "y_test_pred\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred) #compute the confusion matrix between predict label and actual label\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print(f1_score(y_test, y_test_pred, average=\"macro\")) #compute the f1_score between predict label and actual label\n",
        "print(precision_score(y_test, y_test_pred, average=\"macro\")) #compute the precision score between predict label and actual label\n",
        "print(recall_score(y_test, y_test_pred, average=\"macro\"))  #compute the recall score between predict label and actual label\n",
        "conf_matrix\n",
        "\n",
        "#score for svm with (0.1,0.001)\n",
        "def calculate_score(gamma_tem, c_tem,X_train_temp,X_test_temp,y_train_temp,y_test_temp):\n",
        "    rbf_kernel_svm_clf = Pipeline((\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma_tem, C=c_tem))#setting up gamma=0.1, C=100\n",
        "        ))\n",
        "    rbf_kernel_svm_clf.fit(X_train_temp,y_train_temp)\n",
        "    y_test_pred = rbf_kernel_svm_clf.predict(X_test_temp)\n",
        "    print(\"the score of\",(gamma_tem,c_tem),\"is\")\n",
        "    print(f1_score(y_test, y_test_pred, average=\"macro\")) #compute the f1_score between predict label and actual label\n",
        "    print(precision_score(y_test, y_test_pred, average=\"macro\")) #compute the precision score between predict label and actual label\n",
        "    print(recall_score(y_test, y_test_pred, average=\"macro\"))  #compute the recall score between predict label and actual label\n",
        "\n",
        "calculate_score(0.1,0.001,X_train, X_test, y_train, y_test)\n",
        "calculate_score(0.1,1000,X_train, X_test, y_train, y_test)\n",
        "calculate_score(10,0.001,X_train, X_test, y_train, y_test)\n",
        "calculate_score(10,1000,X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "RU0QsP2OBarT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the primum parameters\n",
        "\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "# Example of making your own norm.  Also see matplotlib.colors.\n",
        "# From Joe Kington: This one gives two different linear ramps:\n",
        "class MidpointNormalize(Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "C_range = np.logspace(-2, 10, 13) #setting up the C range\n",
        "gamma_range = np.logspace(-9, 3, 13) #setting up the gamma range\n",
        "param_grid = dict(gamma=gamma_range, C=C_range) #The dict() function creates a dictionary.\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) #Stratified ShuffleSplit cross-validator Provides train/test indices to split data in train/test sets.\n",
        "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv) #Exhaustive search over specified parameter values for an estimator.\n",
        "grid.fit(X_reduced, zone1_tag) #apply reduced values and tags of zone 1 to gridSearchCV\n",
        "\n",
        "print(\n",
        "    \"The best parameters are %s with a score of %0.2f\"\n",
        "    % (grid.best_params_, grid.best_score_)\n",
        ")\n",
        "# classifiers = []\n",
        "# for C in C_range:\n",
        "#     for gamma in gamma_range:\n",
        "#         clf = SVC(C=C, gamma=gamma)\n",
        "#         clf.fit(X_reduced, zone1_tag)\n",
        "#         classifiers.append((C, gamma, clf))\n",
        "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))#A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame.\n",
        "plt.figure(figsize=(8, 6)) #plot a 8X6 figure\n",
        "plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95) #setting up figure position\n",
        "plt.imshow( #show up the value of scores in each sub plot\n",
        "    scores,\n",
        "    interpolation=\"nearest\",\n",
        "    cmap='Set1',\n",
        "    norm=MidpointNormalize(vmin=0.2, midpoint=0.92),\n",
        ")\n",
        "plt.xlabel(\"gamma\")#setting name of x label\n",
        "plt.ylabel(\"C\")#setting name of y label\n",
        "plt.colorbar()#setting the color bar\n",
        "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)#setting x axis\n",
        "plt.yticks(np.arange(len(C_range)), C_range)#setting up y axis\n",
        "plt.title(\"Validation accuracy\")# setting up the title\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "jbLNih8kUILY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying model in zone 1 to zone 2\n",
        "#plot zone 2 based on position\n",
        "zone2_x_axis=[];\n",
        "zone2_y_axis=[];\n",
        "zone2_label=[];\n",
        "for n in zone2_number:# putting value of x, y, label of stars in zone 2 in different array\n",
        "  zone2_x_axis.append(df.X[n])\n",
        "  zone2_y_axis.append(df.Y[n])\n",
        "  zone2_label.append(df.membership[n])\n",
        "zone2_values=[]\n",
        "zone2_tag=[]\n",
        "\n",
        "for n in zone2_number:# put the value of stars:x,y,mux,muy into zone2_values, put the values of membership into zone2_tag\n",
        "  zone2_values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "  zone2_tag.append(df.membership[n])\n",
        "zone2_values=scaler.transform(zone2_values)#transform zone2_value by standard scalar int zone1\n",
        "# zone2_values = StandardScaler().fit_transform(zone2_values)\n",
        "\n",
        "PCA_component_zone_2=sig_pca.transform(zone2_values)#transform the values using sig_pca\n",
        "plt.scatter(PCA_component_zone_2[:,0], PCA_component_zone_2[:,1], c=colormap[zone2_tag])#scatter plot data point of PCA of stars in zone 2\n",
        "plt.title(\"visualization of boundary in zone 2 with gamma=0.1, C=1000\")\n",
        "y_tag_zone_2 = rbf_kernel_svm_clf.predict(PCA_component_zone_2) #predict the membership of stars in zone2 using svm\n",
        "print(f1_score(zone2_tag, y_tag_zone_2, average=\"macro\")) #compute the f1_score between predict label and actual label\n",
        "print(precision_score(zone2_tag, y_tag_zone_2, average=\"macro\"))#compute the precision between predict label and actual label\n",
        "print(recall_score(zone2_tag, y_tag_zone_2, average=\"macro\")) #compute the recall between predict label and actual label\n",
        "conf_matrix = confusion_matrix(zone2_tag, y_tag_zone_2) #compute the confusion matrix between predict label and actual label\n",
        "print(conf_matrix)\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.05, 0.05, -0.05, 0.05]) #plot the boundary between membership and non-membership\n",
        "plot_dataset(PCA_component_zone_2, zone2_tag, [-0.05, 0.05, -0.05, 0.05])#plot the data point\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_NYPMDjtV2aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying model in zone 1 to zone 3\n",
        "#plot zone 3 based on position\n",
        "zone3_x_axis=[];\n",
        "zone3_y_axis=[];\n",
        "zone3_label=[];\n",
        "for n in zone3_number:# putting value of x, y, label of stars in zone 3 in different array\n",
        "  zone3_x_axis.append(df.X[n])\n",
        "  zone3_y_axis.append(df.Y[n])\n",
        "  zone3_label.append(df.membership[n])\n",
        "zone3_values=[]\n",
        "zone3_tag=[]\n",
        "\n",
        "for n in zone3_number:# put the value of stars:x,y,mux,muy into zone3_values, put the values of membership into zone3_tag\n",
        "  zone3_values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "  zone3_tag.append(df.membership[n])\n",
        "zone3_values=scaler.transform(zone3_values)#transform zone3_value by standard scalar int zone1\n",
        "# zone3_values = StandardScaler().fit_transform(zone3_values)\n",
        "\n",
        "PCA_component_zone_3=sig_pca.transform(zone3_values)#transform the values using sig_pca\n",
        "plt.scatter(PCA_component_zone_3[:,0], PCA_component_zone_3[:,1], c=colormap[zone3_tag])#scatter plot data point of PCA of stars in zone 3\n",
        "plt.title(\"visualization of boundary in zone 3 with gamma=0.1, C=1000\")\n",
        "y_tag_zone_3 = rbf_kernel_svm_clf.predict(PCA_component_zone_3)#predict the membership of stars in zone3 using svm\n",
        "print(f1_score(zone3_tag, y_tag_zone_3, average=\"macro\"))#compute the f1_score between predict label and actual label\n",
        "print(precision_score(zone3_tag, y_tag_zone_3, average=\"macro\"))#compute the precision between predict label and actual label\n",
        "print(recall_score(zone3_tag, y_tag_zone_3, average=\"macro\")) #compute the recall between predict label and actual label\n",
        "conf_matrix = confusion_matrix(zone3_tag, y_tag_zone_3)#compute the confusion matrix between predict label and actual label\n",
        "print(conf_matrix)\n",
        "print(conf_matrix)\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.05, 0.05, -0.05, 0.05])#plot the boundary between membership and non-membership\n",
        "plot_dataset(PCA_component_zone_3, zone3_tag, [-0.05, 0.05, -0.05, 0.05])#plot the data point\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ6Zf_dkNtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying model in zone 1 to zone 4\n",
        "#plot zone 4 based on position\n",
        "zone4_x_axis=[];\n",
        "zone4_y_axis=[];\n",
        "zone4_label=[];\n",
        "for n in zone4_number:# putting value of x, y, label of stars in zone 3 in different array\n",
        "  zone4_x_axis.append(df.X[n])\n",
        "  zone4_y_axis.append(df.Y[n])\n",
        "  zone4_label.append(df.membership[n])\n",
        "zone4_values=[]\n",
        "zone4_tag=[]\n",
        "\n",
        "for n in zone4_number:# put the value of stars:x,y,mux,muy into zone4_values, put the values of membership into zone4_tag\n",
        "  zone4_values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "  zone4_tag.append(df.membership[n])\n",
        "zone4_values=scaler.transform(zone4_values)#transform zone4_value by standard scalar int zone1\n",
        "# zone3_values = StandardScaler().fit_transform(zone3_values)\n",
        "\n",
        "PCA_component_zone_4=sig_pca.transform(zone4_values)#transform the values using sig_pca\n",
        "plt.scatter(PCA_component_zone_4[:,0], PCA_component_zone_4[:,1], c=colormap[zone4_tag])#scatter plot data point of PCA of stars in zone 4\n",
        "plt.title(\"visualization of boundary in zone 4 with gamma=0.1, C=100\")\n",
        "y_tag_zone_4 = rbf_kernel_svm_clf.predict(PCA_component_zone_4)#predict the membership of stars in zone3 using svm\n",
        "print(f1_score(zone4_tag, y_tag_zone_4, average=\"macro\"))#compute the f1_score between predict label and actual label\n",
        "print(precision_score(zone4_tag, y_tag_zone_4, average=\"macro\"))#compute the precision between predict label and actual label\n",
        "print(recall_score(zone4_tag, y_tag_zone_4, average=\"macro\")) #compute the recall between predict label and actual label\n",
        "conf_matrix = confusion_matrix(zone4_tag, y_tag_zone_4)#compute the confusion matrix between predict label and actual label\n",
        "print(conf_matrix)\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.05, 0.05, -0.05, 0.05])#plot the boundary between membership and non-membership\n",
        "plot_dataset(PCA_component_zone_4, zone4_tag, [-0.05, 0.05, -0.05, 0.05])#plot the data point"
      ],
      "metadata": {
        "id": "Tn5jGtjlOSJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the pca reduction of original data\n",
        "#centernoid is (20.1255, 20.002499999999998)\n",
        "values=[]\n",
        "df['X'] = df['X'] - 20.1255 # transform the x axis to position to the centeroid\n",
        "df['Y']=df['Y']-20.002499999999998 # transform the y axis to position to the centeroid\n",
        "values=df.iloc[:,1:5] #getting value of x.y.mux,muy\n",
        "\n",
        "tag=df.iloc[:,-1] #getting value of membership\n",
        "scaler = StandardScaler() #initialize a standard scalar\n",
        "scaler.fit(values) # fit the scalar with value of x,y, mux,muy\n",
        "\n",
        "\n",
        "values=scaler.transform(values) #transform the values with scalar\n",
        "#using PCA reduce\n",
        "\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0001, fit_inverse_transform=True)\n",
        "values_reduced = rbf_pca.fit_transform(values) # using PCA to transform value\n",
        "plt.scatter(values_reduced[:, 0], values_reduced[:, 1], c=colormap[tag]) # plot the values\n",
        "plt.title(\"visualization of data in inner 4 zones with PCA information\")\n",
        "plt.xlabel(\"principle component 1\")\n",
        "plt.ylabel(\"principle component 2\")"
      ],
      "metadata": {
        "id": "el3xPGOrrbGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGJsp4r-xHdb"
      },
      "outputs": [],
      "source": [
        "#using all data as training set and testng \n",
        "values=[]\n",
        "tag=[]\n",
        "values=df.iloc[:,1:5]#getting value of x.y.mux,muy\n",
        "tag=df.iloc[:,-1]#getting value of membership\n",
        "\n",
        "# for n in zone1_number:\n",
        "#   values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "#   tag.append(df.membership[n])\n",
        "# for n in zone2_number:\n",
        "#   values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "#   tag.append(df.membership[n])\n",
        "# for n in zone3_number:\n",
        "#   values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "#   tag.append(df.membership[n])\n",
        "# for n in zone4_number:\n",
        "#   values.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "#   tag.append(df.membership[n])\n",
        "\n",
        "grid_search.fit(values, tag) #finding out a best parameter and best score for PCA\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "\n",
        "scaler = StandardScaler() # initlalize a scalar\n",
        "scaler.fit(values)# fit the scalar with value of x,y, mux,muy\n",
        "print(scaler.mean_)\n",
        "\n",
        "values=scaler.transform(values)#transform the values with scalar\n",
        "#using PCA reduce\n",
        "\n",
        "\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)#seting up kernel PCA with sigmoid Kernel\n",
        "\n",
        "\n",
        "values_reduced = sig_pca.fit_transform(values)\n",
        "#values_reduced = lin_pca.fit_transform(values)\n",
        "# plt.scatter(values_reduced[:, 0], values_reduced[:, 1], c=colormap[tag])\n",
        "\n",
        "#choosing inner four zone reduced \n",
        "zone_reduced=[]\n",
        "zone_tag=[]\n",
        "#obtain all values and memberships in inner 4 zones\n",
        "for n in zone1_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone2_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone3_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone4_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "\n",
        "zone_reduced=np.array(zone_reduced) #transform datafram into araray\n",
        "plt.scatter(zone_reduced[:, 0], zone_reduced[:, 1], c=colormap[zone_tag]) #plot the reduced value\n",
        "plt.title(\"visualization of boundary in inner 4 zones with PCA information\")\n",
        "#using SVM to classify\n",
        "C_range = np.logspace(-2, 5, 8) #setting up C range\n",
        "gamma_range = np.logspace(-3, 3, 7) #setting up gamma range\n",
        "param_grid = dict(gamma=gamma_range, C=C_range) # The dict() function creates a dictionary.\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) #Stratified ShuffleSplit cross-validator Provides train/test indices to split data in train/test sets.\n",
        "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv) #Exhaustive search over specified parameter values for an estimator.\n",
        "grid.fit(zone_reduced, zone_tag) #fit the grid with values and tag\n",
        "\n",
        "print(\n",
        "    \"The best parameters are %s with a score of %0.2f\"\n",
        "    % (grid.best_params_, grid.best_score_)\n",
        ")\n",
        "\n",
        "values_train, values_test, tags_train, tags_test = train_test_split( #split data in to 40% train data and 60% test data\n",
        "    zone_reduced, zone_tag, test_size=0.6, random_state=42)\n",
        "rbf_kernel_svm_clf = Pipeline((\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=0.1,C=1000))\n",
        "        )) #setting up gamma to 0.1 C to 1000\n",
        "rbf_kernel_svm_clf.fit(values_train, tags_train) #fit classifier with train data\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.025, 0.025, -0.025, 0.025]) #plot boundary\n",
        "plot_dataset(values_train, tags_train, [-0.025, 0.025, -0.025, 0.025]) #plot data point\n",
        "tags_pred = rbf_kernel_svm_clf.predict(values_test) #predict the result of test data\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_matrix = confusion_matrix(tags_test, tags_pred) #compute the confusion matrix of test data\n",
        "\n",
        "print(f1_score(tags_test, tags_pred, average=\"macro\")) #compute the f1 score of test data\n",
        "print(precision_score(tags_test, tags_pred, average=\"macro\")) #compute the precision score of test data\n",
        "print(recall_score(tags_test, tags_pred, average=\"macro\"))  #compute the recall score of test data\n",
        "conf_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#setting up different kernel of PCA\n",
        "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.01, fit_inverse_transform=True)\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.01, coef0=1, fit_inverse_transform=True)\n",
        "#loop over methods and plot the projection of the Swiss roll\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.suptitle(\"visualization of data with PCA information using different kernel\")\n",
        "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), \n",
        "                            (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n",
        "                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
        "    X_reduced = pca.fit_transform(values)\n",
        "    if subplot == 132:\n",
        "        X_reduced_rbf = X_reduced\n",
        "    \n",
        "    plt.subplot(subplot), plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=colormap[tag])\n",
        "    plt.ylim(-0.1, 0.1)\n",
        "    plt.xlabel('PCA component 1')\n",
        "    plt.ylabel('PCA component 2')"
      ],
      "metadata": {
        "id": "HGozrzUB8tfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qSypX-7XgS_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using model to test zone 1\n",
        "zone1_reduced=[]\n",
        "for n in zone1_number: # collect values in zone 1\n",
        "  zone1_reduced.append(values_reduced[n])\n",
        "  \n",
        "\n",
        "y_zone1_pred = rbf_kernel_svm_clf.predict(zone1_reduced) #making predict using svm classifier\n",
        "conf_matrix = confusion_matrix(zone1_tag, y_zone1_pred) #compute the confusion matrix of test data\n",
        "\n",
        "\n",
        "\n",
        "print(f1_score(zone1_tag, y_zone1_pred, average=\"macro\")) #compute the f1 score of test data\n",
        "print(precision_score(zone1_tag, y_zone1_pred, average=\"macro\"))#compute the precision score of test data\n",
        "print(recall_score(zone1_tag, y_zone1_pred, average=\"macro\"))  #compute the recall score of test data\n",
        "conf_matrix\n",
        "# plot_predictions(rbf_kernel_svm_clf, [-1, 1, -1, 1])\n",
        "# plot_dataset(X_reduced, zone1_tag, [-1, 1, -1, 1])\n",
        "\n",
        "#using model to test zone 2\n",
        "zone2_reduced=[]\n",
        "zone2_tag=[]\n",
        "for n in zone2_number:# collect values in zone 2\n",
        "  zone2_reduced.append(values_reduced[n])\n",
        "  zone2_tag.append(tag[n])\n",
        "zone2_reduced=np.array(zone2_reduced)\n",
        "y_zone2_pred = rbf_kernel_svm_clf.predict(zone2_reduced)#making predict using svm classifier\n",
        "conf_matrix = confusion_matrix(zone2_tag, y_zone2_pred)#compute the confusion matrix of test data\n",
        "\n",
        "\n",
        "\n",
        "print(f1_score(zone2_tag, y_zone2_pred, average=\"macro\"))#compute the f1 score of test data\n",
        "print(precision_score(zone2_tag, y_zone2_pred, average=\"macro\"))#compute the precision score of test data\n",
        "print(recall_score(zone2_tag, y_zone2_pred, average=\"macro\"))  #compute the recall score of test data\n",
        "conf_matrix\n",
        "plot_predictions(rbf_kernel_svm_clf, [-1, 1, -1, 1]) #plot boundry of data point in zone 2\n",
        "plot_dataset(zone2_reduced, zone2_tag, [-1, 1, -1, 1]) #plot data point in zone 2 \n",
        "#using model to test zone 3\n",
        "zone3_reduced=[]\n",
        "zone3_tag=[]\n",
        "for n in zone3_number:# collect values in zone 3\n",
        "  zone3_reduced.append(values_reduced[n])\n",
        "  zone3_tag.append(tag[n])\n",
        "zone3_reduced=np.array(zone3_reduced)\n",
        "y_zone3_pred = rbf_kernel_svm_clf.predict(zone3_reduced)#making predict using svm classifier\n",
        "conf_matrix = confusion_matrix(zone3_tag, y_zone3_pred)#compute the confusion matrix of test data\n",
        "\n",
        "\n",
        "\n",
        "print(f1_score(zone3_tag, y_zone3_pred, average=\"macro\"))#compute the f1 score of test data\n",
        "print(precision_score(zone3_tag, y_zone3_pred, average=\"macro\"))#compute the precision score of test data\n",
        "print(recall_score(zone3_tag, y_zone3_pred, average=\"macro\"))  #compute the recall score of test data\n",
        "conf_matrix\n",
        "plot_predictions(rbf_kernel_svm_clf, [-1, 1, -1, 1])#plot boundry of data point in zone 3\n",
        "plot_dataset(zone3_reduced, zone3_tag, [-1, 1, -1, 1])#plot data point in zone 3\n",
        "\n",
        "#using model to test zone 4\n",
        "zone4_reduced=[]\n",
        "zone4_tag=[]\n",
        "for n in zone4_number:# collect values in zone 4\n",
        "  zone4_reduced.append(values_reduced[n])\n",
        "  zone4_tag.append(tag[n])\n",
        "zone4_reduced=np.array(zone4_reduced)\n",
        "y_zone4_pred = rbf_kernel_svm_clf.predict(zone4_reduced)#making predict using svm classifier\n",
        "conf_matrix = confusion_matrix(zone4_tag, y_zone4_pred)#compute the confusion matrix of test data\n",
        "\n",
        "\n",
        "\n",
        "print(f1_score(zone4_tag, y_zone4_pred, average=\"macro\"))#compute the f1 score of test data\n",
        "print(precision_score(zone4_tag, y_zone4_pred, average=\"macro\"))#compute the precision score of test data\n",
        "print(recall_score(zone4_tag, y_zone4_pred, average=\"macro\"))  #compute the recall score of test data\n",
        "conf_matrix\n",
        "plot_predictions(rbf_kernel_svm_clf, [-0.025, 0.025, -0.025, 0.025])#plot boundry of data point in zone 4\n",
        "plot_dataset(zone4_reduced, zone4_tag, [-0.025, 0.025, -0.025, 0.025])#plot data point in zone 4\n",
        "plt.title(\"visualization of boundary and data point in inner four zones\")"
      ],
      "metadata": {
        "id": "Wf4tQD5hXy37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZBN4OcG-7o"
      },
      "source": [
        "classify by ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4ER7rCwG-7o"
      },
      "outputs": [],
      "source": [
        "# disable convergence warning from early stopping\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)#setting up a Multi layer percetron classifier\n",
        "\n",
        "history_pca=mlp.fit(values_train, tags_train) #train the classifier with train data\n",
        "print(\"Training set score: %f\" % mlp.score(values_train, tags_train)) #print the training score\n",
        "print(\"Test set score: %f\" % mlp.score(values_test, tags_test))#print the testing score\n",
        "loss_value_pca = mlp.loss_curve_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "#not using standard scalar\n",
        "#using all data as training set and testng \n",
        "values=[]\n",
        "tag=[]\n",
        "values=df.iloc[:,1:5] #collecting values\n",
        "tag=df.iloc[:,-1] #collecting labels\n",
        "\n",
        "#using PCA reduce\n",
        "\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)#seting up kernel PCA with sigmoid Kernel\n",
        "\n",
        "\n",
        "sig_pca.fit(values)\n",
        "start = timeit.default_timer()#setting the start time\n",
        "\n",
        "values_reduced = sig_pca.fit_transform(values)#appplying pca to values\n",
        "\n",
        "stop = timeit.default_timer()#setting the stop time\n",
        "print('Time(PCA): ', stop - start)  #print the running time of PCA\n",
        "\n",
        "# plt.scatter(values_reduced[:, 0], values_reduced[:, 1], c=colormap[tag])\n",
        "\n",
        "#choosing inner four zone reduced \n",
        "#collecting data and tags in four zones\n",
        "zone_reduced=[]\n",
        "zone_tag=[]\n",
        "for n in zone1_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone2_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone3_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "for n in zone4_number:\n",
        "  zone_reduced.append(values_reduced[n])\n",
        "  zone_tag.append(tag[n])\n",
        "\n",
        "zone_reduced=np.array(zone_reduced)#transform data frame to array\n",
        "plt.scatter(zone_reduced[:, 0], zone_reduced[:, 1], c=colormap[zone_tag]) #plot data point\n",
        "\n",
        "\n",
        "\n",
        "#using SVM to classify\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "C_range = np.logspace(-2, 5, 8) #setting up C range\n",
        "gamma_range = np.logspace(-3, 3, 7)#setting up gamma range\n",
        "param_grid = dict(gamma=gamma_range, C=C_range) #The dict() function creates a dictionary\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) #Stratified ShuffleSplit cross-validator Provides train/test indices to split data in train/test sets.\n",
        "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv) #Exhaustive search over specified parameter values for an estimator.\n",
        "grid.fit(zone_reduced, zone_tag) #train the grid with reduced value and tag\n",
        "\n",
        "\n",
        "values_train, values_test, tags_train, tags_test = train_test_split(\n",
        "    zone_reduced, zone_tag, test_size=0.5, random_state=42) #split value into 50% train value and 50% testing value\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1) #initialize MLP \n",
        "\n",
        "start = timeit.default_timer()#setting the start time\n",
        "history_pca=mlp.fit(values_train, tags_train)\n",
        "stop = timeit.default_timer()#setting the stop time\n",
        "print(\"Training set score: %f\" % mlp.score(values_train, tags_train)) #print the training set score\n",
        "print(\"Test set score: %f\" % mlp.score(values_test, tags_test)) #print the testing set score\n",
        "loss_value_pca = mlp.loss_curve_\n",
        "plt.title(\"visualization of data point with PCA information without using standard scalar\")\n",
        "plt.xlabel(\"principle component 1\")\n",
        "plt.ylabel(\"principle component 2\")\n",
        "print('Time(with PCA): ', stop - start)  #print the running time of training MLP\n",
        "\n",
        "#calculate time for training time without PCA\n",
        "zone_value=[]\n",
        "\n",
        "for n in zone1_number:\n",
        "  zone_value.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "for n in zone2_number:\n",
        "  zone_value.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "for n in zone3_number:\n",
        "  zone_value.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "for n in zone4_number:\n",
        "  zone_value.append([df.X[n],df.Y[n],df.MUX[n],df.MUY[n]])\n",
        "\n",
        "values_train, values_test, tags_train, tags_test = train_test_split(\n",
        "    zone_value, zone_tag, test_size=0.5, random_state=42) #split value into 50% train value and 50% testing value\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1) #initialize MLP \n",
        "\n",
        "start = timeit.default_timer()#setting the start time\n",
        "mlp.fit(values_train, tags_train)\n",
        "stop = timeit.default_timer()#setting the stop time\n",
        "print('Time(without PCA): ', stop - start)  #print the running time of training MLP"
      ],
      "metadata": {
        "id": "Hz1nVGPm3_tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL7W-cceG-7p"
      },
      "source": [
        "comparing to that using original without PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpH-USZzG-7p"
      },
      "outputs": [],
      "source": [
        "values_comparison_train, values_comparison_test, tags_comparison_train, tags_comparison_test = train_test_split(\n",
        "    values, tag, test_size=0.5, random_state=42)  #split value into 50% train value and 50% testing value\n",
        "start = timeit.default_timer()#setting the start time\n",
        "mlp.fit(values_comparison_train, tags_comparison_train)\n",
        "stop = timeit.default_timer()#setting the stop time\n",
        "print('Time: ', stop - start)#print the running time of training MLP\n",
        "print(\"Training set score: %f\" % mlp.score(values_comparison_train, tags_comparison_train))#print the training set score\n",
        "print(\"testing set score: %f\" % mlp.score(values_comparison_test, tags_comparison_test))#print the testing set score\n",
        "loss_values = mlp.loss_curve_\n",
        "plt.plot(loss_values,label=\"without PCA\") #plot the loss curve without PCA\n",
        "plt.plot(loss_value_pca,label=\"with PCA\") #plot the loss curve with PCA\n",
        "plt.title(\"loss function comparison between using PCA and not using PCA\")\n",
        "plt.xlabel(\"iteration number\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "values_train, values_test, tags_train, tags_test = train_test_split(\n",
        "    zone_reduced, zone_tag, test_size=0.5, random_state=42) #split value into 50% train value and 50% testing value\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[2, 1]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(2, activation=\"softmax\")\n",
        "]) #initialize keras layers\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"]) #setting up parameters\n",
        "print((values_test,tags_test))\n",
        "# fashion_mnist = keras.datasets.fashion_mnist\n",
        "# (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "# X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "# y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "# X_test = X_test / 255.\n",
        "# print((X_valid, y_valid))\n",
        "values_train=np.array(values_train) #transform datafram into arrays\n",
        "print(values_train.shape)\n",
        "values_test=np.array(values_test)#transform datafram into arrays\n",
        "tags_train=np.array(tags_train)#transform datafram into arrays\n",
        "tags_test=np.array(tags_test)#transform datafram into arrays\n",
        "history = model.fit(values_train, tags_train, epochs=10,\n",
        "                    validation_data=(values_test, tags_test))#train the model with training data\n",
        "model.predict(values_train)#predict the result of trainning data"
      ],
      "metadata": {
        "id": "eMTw3g2JzCvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot pca_reducation of new data\n",
        "\n",
        "filename = 'data.txt'\n",
        "data = np.loadtxt(filename, delimiter=',', skiprows=1, dtype=str)\n",
        "print(data)\n",
        "print(float(data[1].split()[1]))\n",
        "new_data=[]\n",
        "\n",
        "for n in data:\n",
        "  \n",
        "  new_x=float(n.split()[0])-289.148202\n",
        "  new_y=float(n.split()[1])-30.183466\n",
        "  new_x_motion=float(n.split()[3])\n",
        "  new_y_motion=float(n.split()[5])\n",
        "  new_data.append([new_x,new_y,new_x_motion,new_y_motion])\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standard_new_data = scaler.transform(new_data)\n",
        "\n",
        "principalComponents_new_data = pca.transform(standard_new_data)\n",
        "count=0\n",
        "\n",
        "print(principalComponents_new_data)\n",
        "plt.scatter(principalComponents_new_data[:, 0], principalComponents_new_data[:, 1])\n",
        "\n",
        "# for n in principalComponents_new_data:\n",
        "#   count=count+1\n",
        "  \n",
        "#   if n[0]>1:\n",
        "#     print(count)\n",
        "#   if n[0]<-1:\n",
        "#     print(count)\n",
        "\n",
        "#it turn out there are 2 extreme large data effect the whole data"
      ],
      "metadata": {
        "id": "sARvLIEysouP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove extreme data from orginal data\n",
        "cropped_new_data=[]\n",
        "count=0\n",
        "for n in new_data:\n",
        "  if count==4127:\n",
        "    pass\n",
        "  elif count==5953:\n",
        "    pass\n",
        "  else:\n",
        "    cropped_new_data.append(n)\n",
        "  count=count+1\n",
        "standard_cropped_new_data = StandardScaler().fit_transform(cropped_new_data)\n",
        "\n",
        "principalComponents_cropped_new_data = pca.transform(standard_cropped_new_data)\n",
        "\n",
        "plt.scatter(principalComponents_cropped_new_data[:, 0], principalComponents_cropped_new_data[:, 1])\n"
      ],
      "metadata": {
        "id": "s4pcEsIK2Nmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the pca reduction of original data\n",
        "#centernoid is (20.1255, 20.002499999999998)\n",
        "values=[]\n",
        "df['X'] = df['X'] - 20.1255 # transform the x axis to position to the centeroid\n",
        "df['Y']=df['Y']-20.002499999999998 # transform the y axis to position to the centeroid\n",
        "values=df.iloc[:,1:5] #getting value of x.y.mux,muy\n",
        "\n",
        "tag=df.iloc[:,-1] #getting value of membership\n",
        "scaler = StandardScaler() #initialize a standard scalar\n",
        "scaler.fit(values) # fit the scalar with value of x,y, mux,muy\n",
        "\n",
        "\n",
        "values=scaler.transform(values) #transform the values with scalar\n",
        "#using PCA reduce\n",
        "\n",
        "\n",
        "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)#seting up kernel PCA with sigmoid Kernel\n",
        "values_reduced = sig_pca.fit_transform(values) # using PCA to transform value\n",
        "\n",
        "classes = ['non-cluster membership[1] ', 'cluster membership[1] ']\n",
        "\n",
        "\n",
        "scatter_plot_c(plt,values_reduced[:, 0],values_reduced[:, 1],\"principal component 1\",\"principal component 1\",\"visualization of old data with PCA information\",tag,colors,classes)"
      ],
      "metadata": {
        "id": "NI4yoeaLqolS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1XONDYFG-7q"
      },
      "source": [
        "classify by CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2axqB3XyG-7r",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from numpy.core.fromnumeric import size\n",
        "\n",
        "filename = 'data.txt'\n",
        "data = np.loadtxt(filename, delimiter=',', skiprows=1, dtype=str)\n",
        "# print(data)\n",
        "# print(float(data[1].split()[1]))\n",
        "new_data=[]\n",
        "\n",
        "for n in data:\n",
        "  \n",
        "  new_x=float(n.split()[0])\n",
        "  new_y=float(n.split()[1])\n",
        "  new_x_motion=float(n.split()[3])\n",
        "  new_y_motion=float(n.split()[5])\n",
        "  new_data_id=int(n.split()[2])\n",
        "  new_data.append([new_x,new_y,new_x_motion,new_y_motion,new_data_id])\n",
        "# print(new_data)\n",
        "#taking 4 arcminutes data, center is (289.148202 +30.183466)\n",
        "arc_minute_new_data=[]\n",
        "new_minute_data_id=[]\n",
        "for n in new_data:\n",
        "  if ((n[0]-289.148202)**2+(n[1]-30.183466)**2)<=0.0044444:\n",
        "    arc_minute_new_data.append(n)\n",
        "    new_minute_data_id.append(n[4])\n",
        "# print(size(new_data))\n",
        "# print(arc_minute_new_data)\n",
        "# print(size(arc_minute_new_data))\n",
        "\n",
        "#transform degree into minimutes\n",
        "miniminutes_new_data=[]\n",
        "for n in arc_minute_new_data:\n",
        "  temp=[(n[0]-289.148202)*60,(n[1]-30.183466)*60,n[2],n[3]]\n",
        "  miniminutes_new_data.append(temp)\n",
        "# print(miniminutes_new_data)\n",
        "#plot new data\n",
        "\n",
        "standard_minutes_new_data=StandardScaler().fit_transform(miniminutes_new_data)\n",
        "principalComponents_minutes_new_data = sig_pca.transform(standard_minutes_new_data)\n",
        "# plt.scatter(principalComponents_minutes_new_data[:, 0], principalComponents_minutes_new_data[:, 1])\n",
        "# plt.xlabel(\"component 1\")#setting name of x label\n",
        "# plt.ylabel(\"component 2\")#setting name of y label\n",
        "# plt.title('visualization of principle compnent of new data')\n",
        "# print(principalComponents_minutes_new_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#predict with svm\n",
        "tags_pred_minute_new_Data = rbf_kernel_svm_clf.predict(principalComponents_minutes_new_data)\n",
        "print(tags_pred_minute_new_Data)\n",
        "\n",
        "prediction=[]\n",
        "\n",
        "count=0\n",
        "number_of_member=0\n",
        "for n in tags_pred_minute_new_Data:\n",
        "  temp=[new_minute_data_id[count],n]\n",
        "  if n==1:\n",
        "    number_of_member=number_of_member+1\n",
        "  count=count+1\n",
        "  prediction.append(temp)\n",
        "print(number_of_member)\n",
        "with open(\"prediction.txt\", \"w+\") as f:\n",
        "  for i in prediction:\n",
        "        f.write(str(i))\n",
        "        f.write('\\n')\n",
        "\n",
        "colors = ListedColormap(['b','r'])\n",
        "\n",
        "classes = ['non-cluster membership ', 'cluster membership ']\n",
        "\n",
        "scatter_plot_c(plt,principalComponents_minutes_new_data[:, 0],principalComponents_minutes_new_data[:, 1],\"principal component 1\",\"principal component 1\",\"visualization of new data with PCA information\",tags_pred_minute_new_Data,colors,classes)\n",
        "plt.axis([-0.05, 0.05, -0.05, 0.05])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the time cost for dimensionality"
      ],
      "metadata": {
        "id": "vvHjRoF_a0iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dpoRIKr4ZnBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAlo0pluG-7r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}